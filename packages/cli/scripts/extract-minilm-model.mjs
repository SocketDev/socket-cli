/**
 * Extract MiniLM model from socketbin-cli-ai and create minilm-sync.mjs
 * This runs during build to extract the compressed model binary from
 * the socketbin-cli-ai package and inline it into the CLI bundle.
 *
 * The model is already INT4-quantized and brotli-compressed by socketbin-cli-ai.
 * We just need to inline the base64-encoded compressed data.
 *
 * Idempotent: Skips regeneration if source hasn't changed (supports CI caching).
 */

import { existsSync, readFileSync, writeFileSync } from 'node:fs'
import path from 'node:path'
import { fileURLToPath } from 'node:url'

import {
  ensureOutputDir,
  generateHashComment,
  shouldExtract,
} from '@socketsecurity/build-infra/lib/extraction-cache'
import { logger } from '@socketsecurity/lib/logger'

const __dirname = path.dirname(fileURLToPath(import.meta.url))
const rootPath = path.join(__dirname, '..')
const outputPath = path.join(rootPath, 'build/minilm-sync.mjs')

// Read the source model path.
const aiBzPath = path.join(rootPath, '../socketbin-cli-ai/dist/ai.bz')

// Check if extraction needed (hash-based caching).
if (
  !(await shouldExtract({
    sourcePaths: aiBzPath,
    outputPath,
    validateOutput: content =>
      content.includes('loadModelSync') &&
      content.includes('brotliDecompressSync'),
  }))
) {
  process.exit(0)
}

if (!existsSync(aiBzPath)) {
  // Graceful fallback: Generate placeholder for CI builds without model.
  logger.warn('socketbin-cli-ai not built yet, generating placeholder')

  const placeholderContent = `/**
 * MiniLM Model Loader (Placeholder)
 *
 * This file is AUTO-GENERATED by scripts/extract-minilm-model.mjs
 * DO NOT EDIT MANUALLY - changes will be overwritten on next build.
 *
 * NOTE: This is a placeholder build. Run 'pnpm build' in socketbin-cli-ai first.
 */

export function loadModelSync() {
  return new ArrayBuffer(0)
}

export function loadTokenizerSync() {
  return {
    model: {
      vocab: {},
    },
    normalizer: {
      lowercase: true,
    },
  }
}
`

  ensureOutputDir(outputPath)
  writeFileSync(outputPath, placeholderContent, 'utf-8')
  logger.log(`✓ Generated placeholder ${outputPath}`)
  process.exit(0)
}

// Read the base64-encoded brotli-compressed model.
const compressedModel = readFileSync(aiBzPath, 'utf-8')

// Compute source hash for cache validation.
const sourceHashComment = await generateHashComment(aiBzPath)

logger.log(
  `✓ Extracted ${compressedModel.length} bytes of compressed model from socketbin-cli-ai`,
)

// Generate minilm-sync.mjs with inlined compressed model.
// TODO: Once we revisit @socketbin/cli-ai for larger payloads, we can extract
// tokenizer separately. For now, the entire model+tokenizer is in ai.bz.
const minilmSyncContent = `/**
 * MiniLM Model and Tokenizer Loader
 *
 * This file is AUTO-GENERATED by scripts/extract-minilm-model.mjs
 * DO NOT EDIT MANUALLY - changes will be overwritten on next build.
 *
 * Provides synchronous loaders for MiniLM INT4-quantized model.
 * The model is brotli-compressed and base64-encoded (from @socketbin/cli-ai).
 *
 * ${sourceHashComment}
 */

import { brotliDecompressSync } from 'node:zlib'

// Inlined compressed model from socketbin-cli-ai (base64-encoded brotli).
const compressedModelBase64 = '${compressedModel}'

// Decode and decompress on first access (lazy initialization).
let decompressedModel = null

/**
 * Load MiniLM INT4 model bytes synchronously.
 * Returns ONNX model binary data for ONNX Runtime.
 */
export function loadModelSync() {
  if (!decompressedModel) {
    const compressed = Buffer.from(compressedModelBase64, 'base64')
    decompressedModel = brotliDecompressSync(compressed)
  }
  return decompressedModel.buffer
}

/**
 * Load tokenizer configuration synchronously.
 * Returns WordPiece tokenizer vocabulary and normalization rules.
 *
 * TODO: Once we revisit @socketbin/cli-ai for larger payloads,
 * we'll extract tokenizer.json separately. For now it's bundled in ai.bz.
 */
export function loadTokenizerSync() {
  // Tokenizer is bundled with the model in ai.bz.
  // This is a stub until we extract it separately.
  return {
    model: {
      vocab: {},
    },
    normalizer: {
      lowercase: true,
    },
  }
}
`

ensureOutputDir(outputPath)
writeFileSync(outputPath, minilmSyncContent, 'utf-8')

logger.log(`✓ Generated ${outputPath}`)
logger.log(`✓ minilm-sync.mjs size: ${minilmSyncContent.length} bytes`)
