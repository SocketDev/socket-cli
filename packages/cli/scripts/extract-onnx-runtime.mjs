/**
 * Extract ONNX Runtime WASM and create onnx-sync.mjs
 * This runs during build to extract the ONNX Runtime WASM binary from
 * our custom build and generate a synchronous loader wrapper.
 *
 * Idempotent: Skips regeneration if source hasn't changed (supports CI caching).
 */

import { existsSync, readFileSync, writeFileSync } from 'node:fs'
import path from 'node:path'
import { fileURLToPath } from 'node:url'

import {
  ensureOutputDir,
  generateHashComment,
  shouldExtract,
} from '@socketsecurity/build-infra/lib/extraction-cache'
import { logger } from '@socketsecurity/lib/logger'

const __dirname = path.dirname(fileURLToPath(import.meta.url))
const rootPath = path.join(__dirname, '..')
const outputPath = path.join(rootPath, 'build/onnx-sync.mjs')

// Source files from custom onnxruntime package.
const onnxPackageRoot = path.join(rootPath, '../onnxruntime')
const onnxWasmFile = path.join(onnxPackageRoot, 'build/wasm/ort-wasm-simd-threaded.wasm')
const onnxJsFile = path.join(onnxPackageRoot, 'build/wasm/ort-wasm-simd-threaded.js')

// Check if extraction needed (hash both files).
if (
  !(await shouldExtract({
    sourcePaths: [onnxWasmFile, onnxJsFile],
    outputPath,
    validateOutput: content =>
      content.includes('onnxruntime') &&
      content.includes('InferenceSession') &&
      content.includes('WebAssembly'),
  }))
) {
  process.exit(0)
}

// Check if ONNX Runtime WASM files exist.
if (!existsSync(onnxWasmFile) || !existsSync(onnxJsFile)) {
  // Graceful fallback: Generate placeholder for CI builds without WASM.
  logger.warn('ONNX Runtime WASM not built yet, generating placeholder')

  const placeholderContent = `/**
 * Synchronous ONNX Runtime with embedded WASM binary (Placeholder).
 *
 * This file is AUTO-GENERATED by scripts/extract-onnx-runtime.mjs
 * DO NOT EDIT MANUALLY - changes will be overwritten on next build.
 *
 * NOTE: This is a placeholder build. Run 'pnpm build' in ../onnxruntime first.
 */

// Placeholder ONNX Runtime export with minimal API.
const ort = {
  InferenceSession: {
    create() {
      throw new Error('ONNX Runtime not built - run pnpm build in packages/onnxruntime')
    }
  },
  Tensor: class Tensor {}
}

export const InferenceSession = ort.InferenceSession
export const Tensor = ort.Tensor

export default ort
`

  ensureOutputDir(outputPath)
  writeFileSync(outputPath, placeholderContent, 'utf-8')
  logger.log(`✓ Generated placeholder ${outputPath}`)
  process.exit(0)
}

// Read WASM binary and convert to base64.
const wasmBinary = readFileSync(onnxWasmFile)
const base64Data = wasmBinary.toString('base64')

// Read our custom Emscripten loader (generated by our build).
const onnxJsContent = readFileSync(onnxJsFile, 'utf-8')

// Compute source hash for cache validation.
const sourceHashComment = await generateHashComment([onnxWasmFile, onnxJsFile])

logger.log(
  `✓ Extracted ${wasmBinary.length} bytes of WASM data from custom onnxruntime`,
)

// Generate onnx-sync.mjs using OUR custom loader with OUR custom WASM.
const onnxSyncContent = `/**
 * Synchronous ONNX Runtime with embedded WASM binary.
 *
 * This file is AUTO-GENERATED by scripts/extract-onnx-runtime.mjs
 * DO NOT EDIT MANUALLY - changes will be overwritten on next build.
 *
 * Uses custom-built ONNX Runtime with Emscripten loader.
 * Built with WASM_ASYNC_COMPILATION=0 for synchronous instantiation.
 *
 * ${sourceHashComment}
 */

// Inlined base64 WASM from custom onnxruntime (extracted at build time).
const base64Wasm = '${base64Data}'

// Decode base64 to Uint8Array.
const wasmBinary = Uint8Array.from(atob(base64Wasm), c => c.charCodeAt(0))

// Inlined Emscripten loader from custom onnxruntime build.
${onnxJsContent}

// Synchronously initialize ONNX Runtime with embedded WASM.
const ort = ortWasmThreaded({
  wasmBinary,
  instantiateWasm(imports, successCallback) {
    // Synchronously instantiate WASM module.
    const module = new WebAssembly.Module(wasmBinary)
    const instance = new WebAssembly.Instance(module, imports)
    successCallback(instance, module)
    return instance.exports
  }
})

export const InferenceSession = ort.InferenceSession
export const Tensor = ort.Tensor

export default ort
`

ensureOutputDir(outputPath)
writeFileSync(outputPath, onnxSyncContent, 'utf-8')

logger.log(`✓ Generated ${outputPath}`)
logger.log(`✓ onnx-sync.mjs size: ${onnxSyncContent.length} bytes`)
