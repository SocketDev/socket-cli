/**
 * Extract onnxruntime-web and create onnx-sync.mjs
 * This runs during build to create a wrapper around onnxruntime-web
 * for use in the MiniLM semantic inference engine.
 *
 * Idempotent: Skips regeneration if file already exists (supports CI caching).
 */

import { existsSync, readFileSync, writeFileSync } from 'node:fs'
import path from 'node:path'
import { fileURLToPath } from 'node:url'

const __dirname = path.dirname(fileURLToPath(import.meta.url))
const rootPath = path.join(__dirname, '..')
const outputPath = path.join(rootPath, 'external/onnx-sync.mjs')

// Check if file already exists (supports CI cache).
if (existsSync(outputPath)) {
  try {
    const existing = readFileSync(outputPath, 'utf-8')
    // Verify it's the expected content (not corrupted).
    if (
      existing.includes('onnxruntime-web') &&
      existing.includes('InferenceSession')
    ) {
      console.log(`✓ Using cached ${outputPath}`)
      process.exit(0)
    }
  } catch {}
  // Fall through to regenerate if verification failed.
}

// Generate onnx-sync.mjs as a simple re-export of onnxruntime-web.
const onnxSyncContent = `/**
 * ONNX Runtime wrapper for MiniLM inference.
 *
 * This file is AUTO-GENERATED by scripts/extract-onnx-runtime.mjs
 * DO NOT EDIT MANUALLY - changes will be overwritten on next build.
 *
 * Re-exports onnxruntime-web for use in MiniLM embedding pipeline.
 */

import ort from 'onnxruntime-web'

export const InferenceSession = ort.InferenceSession
export const Tensor = ort.Tensor

export default ort
`

writeFileSync(outputPath, onnxSyncContent, 'utf-8')

console.log(`✓ Generated ${outputPath}`)
console.log(`✓ onnx-sync.mjs size: ${onnxSyncContent.length} bytes`)
