/**
 * Embed MiniLM Model as Synchronous Loader
 *
 * Follows the yoga-layout pattern:
 * 1. Read model and vocabulary files
 * 2. Base64 encode them
 * 3. Generate external/minilm-sync.mjs with embedded data
 * 4. Provide synchronous loading interface
 *
 * WHY BASE64:
 * - Allows embedding binary data in JavaScript files
 * - Works with bundlers (Rollup, etc.)
 * - Compatible with SEA (Single Executable Application)
 * - 33% size overhead acceptable for convenience
 *
 * OUTPUT:
 * external/minilm-sync.mjs containing:
 * - Base64-encoded tokenizer vocabulary (~900KB)
 * - Base64-encoded ONNX model weights (~22MB)
 * - Synchronous loading utilities
 * - Total: ~23MB (will compress to ~15MB with brotli)
 */

import { readFileSync, writeFileSync } from 'node:fs'
import path from 'node:path'
import { fileURLToPath } from 'node:url'

const __dirname = path.dirname(fileURLToPath(import.meta.url))
const rootPath = path.join(__dirname, '../..')
const cacheDir = path.join(rootPath, '.cache/models')

console.log('â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—')
console.log('â•‘   Embed MiniLM Model for Socket CLI              â•‘')
console.log('â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n')

// Read tokenizer vocabulary.
console.log('ğŸ“– Reading tokenizer.json...')
const tokenizerPath = path.join(cacheDir, 'tokenizer.json')
const tokenizerData = readFileSync(tokenizerPath)
const tokenizerBase64 = tokenizerData.toString('base64')
console.log(`   âœ“ Read ${tokenizerData.length} bytes`)
console.log(`   âœ“ Base64 size: ${tokenizerBase64.length} bytes\n`)

// Read ONNX model.
console.log('ğŸ“– Reading model_quantized.onnx...')
const modelPath = path.join(cacheDir, 'model_quantized.onnx')
const modelData = readFileSync(modelPath)
const modelBase64 = modelData.toString('base64')
console.log(`   âœ“ Read ${modelData.length} bytes`)
console.log(`   âœ“ Base64 size: ${modelBase64.length} bytes\n`)

// Generate minilm-sync.mjs.
console.log('ğŸ“ Generating external/minilm-sync.mjs...')

const syncContent = `/**
 * Synchronous MiniLM Model Loader
 *
 * This file is AUTO-GENERATED by scripts/llm/embed-minilm.mjs
 * DO NOT EDIT MANUALLY - changes will be overwritten on next build.
 *
 * Contains:
 * - Base64-encoded tokenizer vocabulary (${(tokenizerBase64.length / 1024).toFixed(2)} KB)
 * - Base64-encoded ONNX model (${(modelBase64.length / 1024 / 1024).toFixed(2)} MB)
 * - Synchronous loading utilities
 *
 * Total embedded size: ${((tokenizerBase64.length + modelBase64.length) / 1024 / 1024).toFixed(2)} MB
 */

/**
 * Embedded tokenizer vocabulary (base64-encoded).
 */
const TOKENIZER_BASE64 = '${tokenizerBase64}'

/**
 * Embedded ONNX model (base64-encoded).
 */
const MODEL_BASE64 = '${modelBase64}'

/**
 * Decode base64 to Uint8Array.
 */
function decodeBase64(base64) {
  const binaryString = atob(base64)
  const bytes = new Uint8Array(binaryString.length)
  for (let i = 0; i < binaryString.length; i++) {
    bytes[i] = binaryString.charCodeAt(i)
  }
  return bytes
}

/**
 * Load tokenizer vocabulary synchronously.
 *
 * @returns Parsed tokenizer configuration
 */
export function loadTokenizerSync() {
  const bytes = decodeBase64(TOKENIZER_BASE64)
  const text = new TextDecoder().decode(bytes)
  return JSON.parse(text)
}

/**
 * Load ONNX model synchronously.
 *
 * @returns ONNX model as Uint8Array
 */
export function loadModelSync() {
  return decodeBase64(MODEL_BASE64)
}

/**
 * Get embedded asset sizes.
 *
 * @returns Size information
 */
export function getEmbeddedSizes() {
  return {
    tokenizer: {
      base64: TOKENIZER_BASE64.length,
      binary: ${tokenizerData.length},
    },
    model: {
      base64: MODEL_BASE64.length,
      binary: ${modelData.length},
    },
    total: {
      base64: TOKENIZER_BASE64.length + MODEL_BASE64.length,
      binary: ${tokenizerData.length + modelData.length},
    },
  }
}
`

const outputPath = path.join(rootPath, 'external/minilm-sync.mjs')
writeFileSync(outputPath, syncContent, 'utf-8')

console.log(`   âœ“ Generated ${outputPath}`)
console.log(`   âœ“ File size: ${(syncContent.length / 1024 / 1024).toFixed(2)} MB\n`)

console.log('â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—')
console.log('â•‘   Embedding Complete                              â•‘')
console.log('â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n')

const totalMB = ((tokenizerBase64.length + modelBase64.length) / 1024 / 1024).toFixed(2)
console.log(`Embedded size: ${totalMB} MB (base64)`)
console.log(`Original size: ${((tokenizerData.length + modelData.length) / 1024 / 1024).toFixed(2)} MB (binary)`)
console.log(`Overhead: ${((tokenizerBase64.length + modelBase64.length) / (tokenizerData.length + modelData.length) * 100 - 100).toFixed(1)}%`)
console.log(`\nEstimated brotli compressed: ~${(totalMB * 0.6).toFixed(2)} MB`)
console.log('\nNext steps:')
console.log('  1. Implement ONNX inference in src/utils/minilm-inference.mts')
console.log('  2. Wire up to src/commands/ask/handle-ask.mts')
