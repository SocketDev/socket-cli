/**
 * Embed MiniLM Model as Synchronous Loader
 *
 * Strategy:
 * 1. Read model and vocabulary files
 * 2. Compress with brotli (best compression)
 * 3. Base64 encode compressed data
 * 4. Generate external/minilm-sync.mjs with embedded data
 * 5. Runtime: decode base64 â†’ decompress brotli â†’ use
 *
 * WHY BROTLI+BASE64:
 * - Brotli: ~40-60% compression on binary data
 * - Base64: Safe for Rollup parser (no special chars)
 * - Much smaller than plain base64 (23MB â†’ ~8-10MB)
 * - Compatible with SEA (Single Executable Application)
 *
 * OUTPUT:
 * external/minilm-sync.mjs containing:
 * - Brotli+base64 tokenizer vocabulary
 * - Brotli+base64 ONNX model weights
 * - Synchronous decompression utilities
 */

import { readFileSync, writeFileSync } from 'node:fs'
import path from 'node:path'
import { fileURLToPath } from 'node:url'
import { brotliCompressSync } from 'node:zlib'

const __dirname = path.dirname(fileURLToPath(import.meta.url))
const rootPath = path.join(__dirname, '../..')
const cacheDir = path.join(rootPath, '.cache/models')

getDefaultLogger().log('â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—')
getDefaultLogger().log('â•‘   Embed MiniLM Model for Socket CLI              â•‘')
getDefaultLogger().log('â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n')

// Read tokenizer vocabulary.
getDefaultLogger().log('ğŸ“– Reading tokenizer.json...')
const tokenizerPath = path.join(cacheDir, 'tokenizer.json')
const tokenizerData = readFileSync(tokenizerPath)
const tokenizerCompressed = brotliCompressSync(tokenizerData)
const tokenizerBase64 = tokenizerCompressed.toString('base64')
getDefaultLogger().log(`   âœ“ Read ${tokenizerData.length} bytes`)
getDefaultLogger().log(
  `   âœ“ Brotli compressed: ${tokenizerCompressed.length} bytes (${((tokenizerCompressed.length / tokenizerData.length) * 100).toFixed(1)}%)`,
)
getDefaultLogger().log(`   âœ“ Base64 encoded: ${tokenizerBase64.length} bytes\n`)

// Read ONNX model.
getDefaultLogger().log('ğŸ“– Reading model_quantized.onnx...')
const modelPath = path.join(cacheDir, 'model_quantized.onnx')
const modelData = readFileSync(modelPath)
const modelCompressed = brotliCompressSync(modelData)
const modelBase64 = modelCompressed.toString('base64')
getDefaultLogger().log(`   âœ“ Read ${modelData.length} bytes`)
getDefaultLogger().log(
  `   âœ“ Brotli compressed: ${modelCompressed.length} bytes (${((modelCompressed.length / modelData.length) * 100).toFixed(1)}%)`,
)
getDefaultLogger().log(`   âœ“ Base64 encoded: ${modelBase64.length} bytes\n`)

// Generate minilm-sync.mjs.
getDefaultLogger().log('ğŸ“ Generating external/minilm-sync.mjs...')

const syncContent = `/**
 * Synchronous MiniLM Model Loader
 *
 * This file is AUTO-GENERATED by scripts/llm/embed-minilm.mjs
 * DO NOT EDIT MANUALLY - changes will be overwritten on next build.
 *
 * Contains:
 * - Brotli-compressed, base64-encoded tokenizer vocabulary
 * - Brotli-compressed, base64-encoded ONNX model
 * - Synchronous decompression utilities
 *
 * Original sizes:
 * - Tokenizer: ${(tokenizerData.length / 1024).toFixed(2)} KB â†’ ${(tokenizerBase64.length / 1024).toFixed(2)} KB (${((tokenizerCompressed.length / tokenizerData.length) * 100).toFixed(1)}% compressed)
 * - Model: ${(modelData.length / 1024 / 1024).toFixed(2)} MB â†’ ${(modelBase64.length / 1024 / 1024).toFixed(2)} MB (${((modelCompressed.length / modelData.length) * 100).toFixed(1)}% compressed)
 *
 * Total embedded size: ${((tokenizerBase64.length + modelBase64.length) / 1024 / 1024).toFixed(2)} MB
 */

import { brotliDecompressSync } from 'node:zlib'
import { getDefaultLogger } from '@socketsecurity/lib/logger'
import colors from 'yoctocolors-cjs'

/**
 * Embedded tokenizer vocabulary (brotli-compressed, base64-encoded).
 */
const TOKENIZER_BASE64 = '${tokenizerBase64}'

/**
 * Embedded ONNX model (brotli-compressed, base64-encoded).
 */
const MODEL_BASE64 = '${modelBase64}'

/**
 * Load tokenizer vocabulary synchronously.
 *
 * @returns Parsed tokenizer configuration
 */
export function loadTokenizerSync() {
  // Decode base64 to Buffer.
  const compressed = Buffer.from(TOKENIZER_BASE64, 'base64')

  // Decompress with brotli.
  const decompressed = brotliDecompressSync(compressed)

  // Parse JSON.
  const text = decompressed.toString('utf-8')
  return JSON.parse(text)
}

/**
 * Load ONNX model synchronously.
 *
 * @returns ONNX model as Uint8Array
 */
export function loadModelSync() {
  // Decode base64 to Buffer.
  const compressed = Buffer.from(MODEL_BASE64, 'base64')

  // Decompress with brotli.
  const decompressed = brotliDecompressSync(compressed)

  // Return as Uint8Array for ONNX Runtime.
  return new Uint8Array(decompressed.buffer, decompressed.byteOffset, decompressed.byteLength)
}

/**
 * Get embedded asset sizes.
 *
 * @returns Size information
 */
export function getEmbeddedSizes() {
  return {
    tokenizer: {
      compressed: ${tokenizerCompressed.length},
      base64: TOKENIZER_BASE64.length,
      original: ${tokenizerData.length},
    },
    model: {
      compressed: ${modelCompressed.length},
      base64: MODEL_BASE64.length,
      original: ${modelData.length},
    },
    total: {
      compressed: ${tokenizerCompressed.length + modelCompressed.length},
      base64: TOKENIZER_BASE64.length + MODEL_BASE64.length,
      original: ${tokenizerData.length + modelData.length},
    },
  }
}
`

const outputPath = path.join(rootPath, 'external/minilm-sync.mjs')
writeFileSync(outputPath, syncContent, 'utf-8')

getDefaultLogger().log(`   âœ“ Generated ${outputPath}`)
getDefaultLogger().log(
  `   âœ“ File size: ${(syncContent.length / 1024 / 1024).toFixed(2)} MB\n`,
)

getDefaultLogger().log('â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—')
getDefaultLogger().log('â•‘   Embedding Complete                              â•‘')
getDefaultLogger().log('â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n')

const originalSizeMB = (
  (tokenizerData.length + modelData.length) /
  1024 /
  1024
).toFixed(2)
const compressedSizeMB = (
  (tokenizerCompressed.length + modelCompressed.length) /
  1024 /
  1024
).toFixed(2)
const base64SizeMB = (
  (tokenizerBase64.length + modelBase64.length) /
  1024 /
  1024
).toFixed(2)

getDefaultLogger().log('ğŸ“Š Compression Results:')
getDefaultLogger().log(`   Original:    ${originalSizeMB} MB`)
getDefaultLogger().log(
  `   Compressed:  ${compressedSizeMB} MB (${(((tokenizerCompressed.length + modelCompressed.length) / (tokenizerData.length + modelData.length)) * 100).toFixed(1)}%)`,
)
getDefaultLogger().log(`   Base64:      ${base64SizeMB} MB`)
getDefaultLogger().log('')
getDefaultLogger().log(
  `   Total savings: ${(originalSizeMB - base64SizeMB).toFixed(2)} MB (${(100 - (base64SizeMB / originalSizeMB) * 100).toFixed(1)}% reduction)`,
)
getDefaultLogger().log('\nNext steps:')
getDefaultLogger().log('  1. Run build: pnpm run build')
getDefaultLogger().log('  2. Test LLM features in src/commands/ask/handle-ask.mts')
